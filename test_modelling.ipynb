{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmbd = 0.1\n",
    "embed_size = 150\n",
    "dropout_rate = 0.5\n",
    "K = 3\n",
    "epochs = 1\n",
    "number_of_item = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mhypernetx\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mhp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcollections\u001b[39;00m \u001b[39mimport\u001b[39;00m defaultdict\n",
      "File \u001b[1;32mc:\\python38\\lib\\site-packages\\hypernetx\\__init__.py:10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mhypernetx\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mreports\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mhypernetx\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdrawing\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mhypernetx\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39malgorithms\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mhypernetx\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39malgorithms\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcontagion\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mhypernetx\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[1;32mc:\\python38\\lib\\site-packages\\hypernetx\\algorithms\\__init__.py:4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39ms_centrality_measures\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcontagion\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mlaplacians_clustering\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mgenerative_models\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mhypergraph_modularity\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[1;32mc:\\python38\\lib\\site-packages\\hypernetx\\algorithms\\laplacians_clustering.py:30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msparse\u001b[39;00m \u001b[39mimport\u001b[39;00m csr_matrix, coo_matrix, diags, find, identity\n\u001b[0;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msparse\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinalg\u001b[39;00m \u001b[39mimport\u001b[39;00m eigs\n\u001b[1;32m---> 30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcluster\u001b[39;00m \u001b[39mimport\u001b[39;00m SpectralClustering, KMeans\n\u001b[0;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m \u001b[39mimport\u001b[39;00m preprocessing\n\u001b[0;32m     32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfunctools\u001b[39;00m \u001b[39mimport\u001b[39;00m partial\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\cluster\\__init__.py:6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mThe :mod:`sklearn.cluster` module gathers popular unsupervised clustering\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39malgorithms.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_spectral\u001b[39;00m \u001b[39mimport\u001b[39;00m spectral_clustering, SpectralClustering\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_mean_shift\u001b[39;00m \u001b[39mimport\u001b[39;00m mean_shift, MeanShift, estimate_bandwidth, get_bin_seeds\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_affinity_propagation\u001b[39;00m \u001b[39mimport\u001b[39;00m affinity_propagation, AffinityPropagation\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\cluster\\_spectral.py:20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m check_random_state, as_float_array, check_scalar\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpairwise\u001b[39;00m \u001b[39mimport\u001b[39;00m pairwise_kernels\n\u001b[1;32m---> 20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mneighbors\u001b[39;00m \u001b[39mimport\u001b[39;00m kneighbors_graph, NearestNeighbors\n\u001b[0;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmanifold\u001b[39;00m \u001b[39mimport\u001b[39;00m spectral_embedding\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_kmeans\u001b[39;00m \u001b[39mimport\u001b[39;00m k_means\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\neighbors\\__init__.py:17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_kde\u001b[39;00m \u001b[39mimport\u001b[39;00m KernelDensity\n\u001b[0;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_lof\u001b[39;00m \u001b[39mimport\u001b[39;00m LocalOutlierFactor\n\u001b[1;32m---> 17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_nca\u001b[39;00m \u001b[39mimport\u001b[39;00m NeighborhoodComponentsAnalysis\n\u001b[0;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_base\u001b[39;00m \u001b[39mimport\u001b[39;00m VALID_METRICS, VALID_METRICS_SPARSE\n\u001b[0;32m     20\u001b[0m __all__ \u001b[39m=\u001b[39m [\n\u001b[0;32m     21\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mBallTree\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     22\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mDistanceMetric\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mVALID_METRICS_SPARSE\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     39\u001b[0m ]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\neighbors\\_nca.py:19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseEstimator, TransformerMixin, _ClassNamePrefixFeaturesOutMixin\n\u001b[0;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m LabelEncoder\n\u001b[1;32m---> 19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdecomposition\u001b[39;00m \u001b[39mimport\u001b[39;00m PCA\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmulticlass\u001b[39;00m \u001b[39mimport\u001b[39;00m check_classification_targets\n\u001b[0;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrandom\u001b[39;00m \u001b[39mimport\u001b[39;00m check_random_state\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\decomposition\\__init__.py:16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_incremental_pca\u001b[39;00m \u001b[39mimport\u001b[39;00m IncrementalPCA\n\u001b[0;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_kernel_pca\u001b[39;00m \u001b[39mimport\u001b[39;00m KernelPCA\n\u001b[1;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_sparse_pca\u001b[39;00m \u001b[39mimport\u001b[39;00m SparsePCA, MiniBatchSparsePCA\n\u001b[0;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_truncated_svd\u001b[39;00m \u001b[39mimport\u001b[39;00m TruncatedSVD\n\u001b[0;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_fastica\u001b[39;00m \u001b[39mimport\u001b[39;00m FastICA, fastica\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\decomposition\\_sparse_pca.py:11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m check_random_state\n\u001b[0;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvalidation\u001b[39;00m \u001b[39mimport\u001b[39;00m check_is_fitted\n\u001b[1;32m---> 11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinear_model\u001b[39;00m \u001b[39mimport\u001b[39;00m ridge_regression\n\u001b[0;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseEstimator, TransformerMixin, _ClassNamePrefixFeaturesOutMixin\n\u001b[0;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_dict_learning\u001b[39;00m \u001b[39mimport\u001b[39;00m dict_learning, dict_learning_online\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\linear_model\\__init__.py:9\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mThe :mod:`sklearn.linear_model` module implements a variety of linear models.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39m# See http://scikit-learn.sourceforge.net/modules/sgd.html and\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39m# http://scikit-learn.sourceforge.net/modules/linear_model.html for\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39m# complete documentation.\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_base\u001b[39;00m \u001b[39mimport\u001b[39;00m LinearRegression\n\u001b[0;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_bayes\u001b[39;00m \u001b[39mimport\u001b[39;00m BayesianRidge, ARDRegression\n\u001b[0;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_least_angle\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     12\u001b[0m     Lars,\n\u001b[0;32m     13\u001b[0m     LassoLars,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     LassoLarsIC,\n\u001b[0;32m     19\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\linear_model\\_base.py:38\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mextmath\u001b[39;00m \u001b[39mimport\u001b[39;00m _incremental_mean_and_var\n\u001b[0;32m     37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msparsefuncs\u001b[39;00m \u001b[39mimport\u001b[39;00m mean_variance_axis, inplace_column_scale\n\u001b[1;32m---> 38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_seq_dataset\u001b[39;00m \u001b[39mimport\u001b[39;00m ArrayDataset32, CSRDataset32\n\u001b[0;32m     39\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_seq_dataset\u001b[39;00m \u001b[39mimport\u001b[39;00m ArrayDataset64, CSRDataset64\n\u001b[0;32m     40\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvalidation\u001b[39;00m \u001b[39mimport\u001b[39;00m check_is_fitted, _check_sample_weight\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:389\u001b[0m, in \u001b[0;36mparent\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import hypernetx as hp\n",
    "import re\n",
    "\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from gensim.models import Word2Vec,KeyedVectors\n",
    "\n",
    "class Data ():\n",
    "    def __init__(self):      \n",
    "        self.all_hyper_edges = []\n",
    "        self.all_hyper_graphs = []\n",
    "        self.users = []\n",
    "        self.data = []        \n",
    "        \n",
    "        self.number_of_items = 0 #nodes\n",
    "        self.number_of_users = 0 #edges\n",
    "        self.number_of_interactions = 0\n",
    "        self.static_item_embedding = []\n",
    "        # self.static_item_embedding = torch.Tensor(self.number_of_items,embed_size)\n",
    "\n",
    "    def set_data(self , features_files_name , interactions_files_name):\n",
    "        self.features_dataframe = pd.read_csv(features_files_name)\n",
    "        self.interactions_dataframe = pd.read_csv(interactions_files_name)\n",
    "        self.dataset_by_year = dict(tuple(self.interactions_dataframe.groupby(\"reviewTime\")))\n",
    "        self.item_purched_list , self.year_purched_list , self.user_item_year_list = self.set_purches_list()\n",
    "\n",
    "    def clear_every_thing(self):\n",
    "        self.all_hyper_edges = []\n",
    "        self.all_hyper_graphs = []\n",
    "        self.users = []\n",
    "        self.data = []        \n",
    "        \n",
    "        self.number_of_items = 0 #nodes\n",
    "        self.number_of_users = 0 #edges\n",
    "        self.number_of_interactions = 0\n",
    "        self.static_item_embedding = []\n",
    "\n",
    "\n",
    "\n",
    "    def show_dataset_by_year(self):\n",
    "        for val in self.dataset_by_year:\n",
    "            print(val)\n",
    "            print(self.dataset_by_year[val])\n",
    "            print('\\n')\n",
    "\n",
    "    def get_dataset_by_year(self):\n",
    "        return self.dataset_by_year\n",
    "\n",
    "    def get_years(self):\n",
    "        return self.dataset_by_year.keys()\n",
    "\n",
    "    def get_user(self):\n",
    "        self.users = list(set(self.interactions_dataframe['reviewerID']))\n",
    "        return self.users\n",
    "\n",
    "    def set_purches_list(self):\n",
    "        purches_list = defaultdict(list)\n",
    "        user_year_list = defaultdict(list) # list salhaye kharid karbar \n",
    "        user_item_year_list = defaultdict(list)\n",
    "        for year in range(0,len(self.get_dataset_by_year())):\n",
    "            df_aYear = list(self.get_dataset_by_year().values())[year]\n",
    "            plist = []\n",
    "            for interaction_row in df_aYear.iterrows():\n",
    "                item_id = interaction_row[1]['asin']\n",
    "                user_id = interaction_row[1]['reviewerID']\n",
    "                u_year = interaction_row[1]['reviewTime']\n",
    "                purches_list[user_id].append(item_id)\n",
    "                user_year_list[user_id].append(u_year)\n",
    "                interaction = (item_id,u_year)\n",
    "                plist.append(interaction)\n",
    "            user_item_year_list[user_id].append(plist)\n",
    "            plist = []\n",
    "\n",
    "        return purches_list , user_year_list , user_item_year_list\n",
    "\n",
    "    def get_user_purches_item(self,user_id):\n",
    "        return self.item_purched_list[user_id]\n",
    "\n",
    "    def get_user_purches_year(self,user_id):\n",
    "        return self.year_purched_list[user_id]\n",
    "\n",
    "    def get_user_item_year_list(self):\n",
    "        return self.user_item_year_list\n",
    "\n",
    "    def get_all_hyper_edges(self):\n",
    "        hyper_edge_for_one_hypergraph = {}\n",
    "        edge_number = []\n",
    "        for one_year in self.dataset_by_year:\n",
    "            hyper_edge_for_one_hypergraph = {}\n",
    "            edge_number = []\n",
    "            for ind in self.dataset_by_year[one_year].index:\n",
    "                edge_number.append(self.dataset_by_year[one_year]['reviewerID'][ind])\n",
    "            edge_number = list(dict.fromkeys(edge_number))\n",
    "            nodes = []\n",
    "\n",
    "            for edge in edge_number:\n",
    "                for ind in self.dataset_by_year[one_year].index:        \n",
    "                    if edge==int(self.dataset_by_year[one_year]['reviewerID'][ind]):            \n",
    "                        nodes.append(int(self.dataset_by_year[one_year]['asin'][ind]))\n",
    "                    elif edge > int(self.dataset_by_year[one_year]['reviewerID'][ind]): \n",
    "                        continue\n",
    "                    else:\n",
    "                        hyper_edge_for_one_hypergraph[edge]=nodes\n",
    "                        nodes = []\n",
    "                        break\n",
    "            hyper_edge_for_one_hypergraph[edge]=nodes\n",
    "            self.all_hyper_edges.append(hyper_edge_for_one_hypergraph)\n",
    "        return self.all_hyper_edges\n",
    "\n",
    "    def get_all_hyper_graphs(self):\n",
    "        for i in range(len(self.all_hyper_edges)):\n",
    "            G = hp.Hypergraph(self.all_hyper_edges[i].values())\n",
    "            self.all_hyper_graphs.append(G)\n",
    "        return self.all_hyper_graphs\n",
    "\n",
    "    def removeNonAscii(self,text):\n",
    "        return \"\".join(i for i in text if  ord(i)<128)\n",
    "    \n",
    "    def make_lower_case(self,text):\n",
    "        return text.lower() \n",
    "\n",
    "    def remove_html(self,text):\n",
    "        html_pattern = re.compile('<.*?>')\n",
    "        return html_pattern.sub(r'', text)\n",
    "\n",
    "    def remove_stop_words(self,text):\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "        return text   \n",
    "\n",
    "    def remove_punctuation(self,text):\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        text = tokenizer.tokenize(text)\n",
    "        text = \" \".join(text)  \n",
    "        return text\n",
    "    \n",
    "    def remove_punctuation_from_list(self,list):\n",
    "        string = \"\"\n",
    "        for w in list:\n",
    "            string = \" \".join(w)\n",
    "        return self.remove_punctuation(string)\n",
    "\n",
    "    def cleaning_data(self):\n",
    "        # description\n",
    "        self.features_dataframe = self.features_dataframe[self.features_dataframe['description'].notnull()]\n",
    "        self.features_dataframe['Cleaned_description'] = self.features_dataframe['description'].apply(self.removeNonAscii)\n",
    "        self.features_dataframe['Cleaned_description'] = self.features_dataframe.Cleaned_description.apply(func = self.remove_html)\n",
    "        self.features_dataframe['Cleaned_description'] = self.features_dataframe.Cleaned_description.apply(func = self.make_lower_case)\n",
    "        self.features_dataframe['Cleaned_description'] = self.features_dataframe.Cleaned_description.apply(func = self.remove_stop_words)\n",
    "        self.features_dataframe['Cleaned_description'] = self.features_dataframe.Cleaned_description.apply(func = self.remove_punctuation)\n",
    "\n",
    "        # categories\n",
    "        self.features_dataframe = self.features_dataframe[self.features_dataframe['categories'].notnull()]\n",
    "        self.features_dataframe['Cleaned_categories'] = self.features_dataframe['categories'].apply(self.remove_punctuation_from_list)\n",
    "        self.features_dataframe['Cleaned_categories'] = self.features_dataframe.Cleaned_categories.apply(func = self.make_lower_case)\n",
    "\n",
    "        # title\n",
    "        self.features_dataframe = self.features_dataframe[self.features_dataframe['title'].notnull()]\n",
    "        self.features_dataframe['Cleaned_title'] = self.features_dataframe['title'].apply(self.removeNonAscii)\n",
    "        self.features_dataframe['Cleaned_title'] = self.features_dataframe.Cleaned_title.apply(func = self.make_lower_case)\n",
    "        self.features_dataframe['Cleaned_title'] = self.features_dataframe.Cleaned_title.apply(func = self.remove_stop_words)\n",
    "        self.features_dataframe['Cleaned_title'] = self.features_dataframe.Cleaned_title.apply(func = self.remove_punctuation)\n",
    "\n",
    "        self.features_dataframe['Cleaned_description'] = self.features_dataframe.Cleaned_description.apply(func = self.make_lower_case)\n",
    "        self.features_dataframe['Cleaned_description'] = self.features_dataframe.Cleaned_description.apply(func = self.remove_stop_words)\n",
    "        self.features_dataframe['Cleaned_description'] = self.features_dataframe.Cleaned_description.apply(func = self.remove_punctuation)\n",
    "\n",
    "        # categories\n",
    "        self.features_dataframe = self.features_dataframe[self.features_dataframe['categories'].notnull()]\n",
    "        self.features_dataframe['Cleaned_categories'] = self.features_dataframe['categories'].apply(self.remove_punctuation_from_list)\n",
    "        self.features_dataframe['Cleaned_categories'] = self.features_dataframe.Cleaned_categories.apply(func = self.make_lower_case)\n",
    "\n",
    "        # title\n",
    "        self.features_dataframe = self.features_dataframe[self.features_dataframe['title'].notnull()]\n",
    "        self.features_dataframe['Cleaned_title'] = self.features_dataframe['title'].apply(self.removeNonAscii)\n",
    "        self.features_dataframe['Cleaned_title'] = self.features_dataframe.Cleaned_title.apply(func = self.make_lower_case)\n",
    "        self.features_dataframe['Cleaned_title'] = self.features_dataframe.Cleaned_title.apply(func = self.remove_stop_words)\n",
    "        self.features_dataframe['Cleaned_title'] = self.features_dataframe.Cleaned_title.apply(func = self.remove_punctuation)\n",
    "\n",
    "        description = []\n",
    "        for words in self.features_dataframe['Cleaned_description']:\n",
    "            description.append(words.split())\n",
    "\n",
    "        categories = []\n",
    "        for words in self.features_dataframe['Cleaned_categories']:\n",
    "            categories.append(words.split())\n",
    "\n",
    "        title = []\n",
    "        for words in self.features_dataframe['Cleaned_title']:\n",
    "            title.append(words.split())\n",
    "\n",
    "        self.data = list(zip(title,categories,description))\n",
    "        for i in range(len(self.data)):\n",
    "            self.data[i] = self.data[i][0] + self.data[i][1] + self.data[i][2]\n",
    "\n",
    "    def training_google_model(self):\n",
    "        self.cleaning_data()\n",
    "        EMBEDDING_FILE = 'E:/google news vector/GoogleNews-vectors-negative300.bin.gz'\n",
    "        google_word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary = True)\n",
    "\n",
    "        # Training our corpus with the model\n",
    "        google_model = Word2Vec(size = 300, window = 5, min_count = 2, workers = -1)\n",
    "\n",
    "        google_model.build_vocab(self.data)\n",
    "\n",
    "        google_model.intersect_word2vec_format(EMBEDDING_FILE, lockf = 1.0, binary = True)\n",
    "\n",
    "        google_model.train(self.data, total_examples = google_model.corpus_count, epochs = 5)\n",
    "\n",
    "        return google_model\n",
    "\n",
    "    def create_static_embedding(self):\n",
    "        google = self.training_google_model()\n",
    "        static_item_embeddings = []\n",
    "        for line in self.data:\n",
    "            avgword2vec = None\n",
    "            count = 0\n",
    "            for word in line:\n",
    "                if word in google.wv.vocab:\n",
    "                    count += 1\n",
    "                    if avgword2vec is None:\n",
    "                        avgword2vec = google[word]\n",
    "                    else:\n",
    "                        avgword2vec = avgword2vec + google[word]\n",
    "\n",
    "            if avgword2vec is not None:\n",
    "                avgword2vec = avgword2vec / count\n",
    "                static_item_embeddings.append(avgword2vec)\n",
    "\n",
    "        return static_item_embeddings\n",
    "\n",
    "    def set_static_embedding(self):\n",
    "        self.static_item_embedding = self.create_static_embedding()\n",
    "        \n",
    "        # self.static_item_embedding = static_item_embeddings\n",
    "\n",
    "    def get_static_embedding(self,index):\n",
    "        return self.static_item_embedding[index]\n",
    "\n",
    "    def get_statistic(self):\n",
    "        self.number_of_items = len(pd.unique(self.interactions_dataframe['asin'])) #nodes\n",
    "        self.number_of_users = len(pd.unique(self.interactions_dataframe['reviewerID'])) #edges\n",
    "        self.number_of_interactions = self.interactions_dataframe.shape[0]\n",
    "\n",
    "        return self.number_of_items , self.number_of_users , self.number_of_interactions\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import hypernetx as hp\n",
    "\n",
    "\n",
    "from torch.nn import Parameter , TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "\n",
    "class ResidualGAtingLayer(nn.Module):\n",
    "    def __init__(self,num_item,embed_size):\n",
    "        super(ResidualGAtingLayer, self).__init__()\n",
    "        self.W_R = Parameter(data = torch.FloatTensor(embed_size, embed_size), requires_grad=True)\n",
    "        torch.nn.init.uniform_(self.W_R,-1,1)\n",
    "        self.Z_R = Parameter(data = torch.FloatTensor(embed_size), requires_grad=True)\n",
    "        torch.nn.init.uniform_(self.Z_R,-1,1)\n",
    "        self.G = 0\n",
    "        self.x = torch.empty((num_item,embed_size))\n",
    "        self.e = torch.empty((num_item,embed_size))\n",
    "    \n",
    "    def setter(self,dynamic_embeddings,static_item_embeddings):\n",
    "        self.x = dynamic_embeddings\n",
    "        self.e = static_item_embeddings\n",
    "\n",
    "    def forward(self):\n",
    "        self.G = torch.exp(torch.transpose(self.Z_R, 0,0) * torch.tanh(torch.matmul(self.W_R,self.x))) / (torch.exp(torch.transpose(self.Z_R, 0, 0) * torch.tanh(torch.matmul(self.W_R,self.x))) + torch.exp(torch.transpose(self.Z_R, 0, 0) * torch.tanh(self.W_R*self.e)))\n",
    "        return self.G\n",
    "\n",
    "    def calculate_residualGAting(self):\n",
    "        return self.G*self.x + (1-self.G)*self.e\n",
    "\n",
    "\n",
    "class ShortTermUserIntent(nn.Module):\n",
    "    def __init__(self,num_item , embed_size ):\n",
    "        super(ShortTermUserIntent, self).__init__()\n",
    "        self.W = Parameter(data = torch.Tensor(num_item, embed_size), requires_grad=True)\n",
    "        torch.nn.init.uniform_(self.W,-1,1)\n",
    "\n",
    "        self.item_num = num_item\n",
    "        self.hyperGraph = hp.Hypergraph\n",
    "        self.incident_matrix = np.empty(embed_size)\n",
    "        self.incident_matrix_tensor = torch.empty((num_item,embed_size))\n",
    "        self.H_transpose = torch.empty((num_item,embed_size))       \n",
    "        self.diagonal_Dv_invert = torch.empty((num_item,embed_size))  \n",
    "        self.diagonal_De_invert = torch.empty((num_item,embed_size))  \n",
    "\n",
    "    def setter(self,G):\n",
    "        self.hyperGraph = G\n",
    "        self.incident_matrix = np.matrix(self.hyperGraph.incidence_matrix().toarray())\n",
    "        self.incident_matrix_tensor = torch.tensor(self.hyperGraph.incidence_matrix().toarray()).float()\n",
    "        self.H_transpose = torch.transpose(self.incident_matrix_tensor, 0, 1)\n",
    "        Dv = np.array(np.sum(self.incident_matrix, axis=0)).flatten()\n",
    "        De = np.array(np.sum(self.incident_matrix, axis=1)).flatten()\n",
    "        invertedDv = []\n",
    "        for i in Dv:\n",
    "            if i==0:\n",
    "                invertedDv.append(i)\n",
    "            else:\n",
    "                invertedDv.append(1/np.power(i , 0.5))\n",
    "        Dv_invert = torch.tensor(invertedDv).float()\n",
    "        \n",
    "        self.diagonal_Dv_invert = torch.diag(Dv_invert)\n",
    "\n",
    "        invertedDe = []\n",
    "        for i in De:\n",
    "            if i==0:\n",
    "                invertedDe.append(i)\n",
    "            else:\n",
    "                invertedDe.append(1/np.power(i , 0.5))\n",
    "        De_invert = torch.tensor(invertedDe).float()\n",
    "        self.diagonal_De_invert = torch.diag(De_invert)\n",
    "\n",
    "    def forward(self,X , batch_num):\n",
    "        temp = torch.matmul(self.H_transpose,self.diagonal_De_invert)\n",
    "        temp = torch.matmul(self.diagonal_Dv_invert,temp)\n",
    "\n",
    "        right_index = self.item_num*batch_num \n",
    "\n",
    "        edg = []\n",
    "        for i in list(self.hyperGraph):\n",
    "            edg.append(i-right_index)\n",
    "            \n",
    "\n",
    "        # temp = torch.matmul(temp,X[list(self.hyperGraph)])\n",
    "        temp = torch.matmul(temp,X[edg])\n",
    "        temp = torch.matmul(temp,self.W)\n",
    "        return F.relu(temp)\n",
    "\n",
    "\n",
    "class FusionLayer(nn.Module):\n",
    "    def __init__(self,num_item ,embed_size):\n",
    "        super(FusionLayer, self).__init__()\n",
    "        self.W_R = Parameter(data = torch.FloatTensor(embed_size, embed_size), requires_grad=True)\n",
    "        torch.nn.init.uniform_(self.W_R,-1,1)\n",
    "        self.Z_R = Parameter(data = torch.FloatTensor(embed_size), requires_grad=True)\n",
    "        torch.nn.init.uniform_(self.Z_R,-1,1)\n",
    "        self.a_u = 0\n",
    "        self.a_d = 0\n",
    "        self.x = torch.empty((num_item,embed_size))\n",
    "        self.e = torch.empty((num_item,embed_size))\n",
    "        self.u = torch.empty((num_item,embed_size))\n",
    "    \n",
    "    def setter(self,dynamic_embeddings,static_item_embeddings,user_embedding):\n",
    "        self.x = dynamic_embeddings\n",
    "        self.e = static_item_embeddings\n",
    "        self.u = user_embedding\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        self.a_u = torch.exp(torch.transpose(self.Z_R, 0,0) * torch.tanh(torch.matmul(self.W_R,self.u))) / torch.exp(torch.transpose(self.Z_R, 0,0) * torch.tanh(torch.matmul(self.W_R,self.u))) + torch.exp(torch.transpose(self.Z_R, 0,0) * torch.tanh(torch.matmul(self.W_R,self.x))) +torch.exp(torch.transpose(self.Z_R, 0,0) * torch.tanh(torch.matmul(self.W_R,self.e)))\n",
    "        self.a_d = torch.exp(torch.transpose(self.Z_R, 0,0) * torch.tanh(torch.matmul(self.W_R,self.x))) / torch.exp(torch.transpose(self.Z_R, 0,0) * torch.tanh(torch.matmul(self.W_R,self.u))) + torch.exp(torch.transpose(self.Z_R, 0,0) * torch.tanh(torch.matmul(self.W_R,self.x))) +torch.exp(torch.transpose(self.Z_R, 0,0) * torch.tanh(torch.matmul(self.W_R,self.e)))\n",
    "        return self.a_u , self.a_d\n",
    "\n",
    "    def calculate_FusionLayer(self):\n",
    "        u = list(self.u)\n",
    "        x = list(self.x)\n",
    "        e = list(self.e)\n",
    "\n",
    "        # return self.a_u*self.u + self.a_d*self.x + (1 - self.a_d - self.a_u)*self.e\n",
    "        uau = self.a_u*u\n",
    "        xad = self.a_d*x\n",
    "        e1 = (1 - self.a_d - self.a_u)*e\n",
    "\n",
    "        return torch.Tensor(uau+xad+e1)\n",
    "\n",
    "\n",
    "class PositionalEncodingLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.n=10000\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        # self.P = torch.zeros((seq_len, embeding_size))\n",
    "\n",
    "    def calculatePositionEncoding(self,seq_len):\n",
    "        pe = torch.zeros((seq_len, embed_size))\n",
    "        for k in range(seq_len):\n",
    "            for i in np.arange(int(embed_size/2)):\n",
    "                denominator = np.power(self.n, 2*i/embed_size)\n",
    "                pe[k, 2*i] = np.sin(k/denominator)\n",
    "                pe[k, 2*i+1] = np.cos(k/denominator)\n",
    "        return pe \n",
    "\n",
    "    def forward(self, seq_len ,x):\n",
    "        x = x + self.calculatePositionEncoding(seq_len)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self,d_model, nhead, d_hid, nlayers ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncodingLayer()\n",
    "        encoder_layers = TransformerEncoderLayer(self.d_model, nhead, d_hid, 0.5)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.decoder = nn.Linear(self.d_model, self.d_model)\n",
    "\n",
    "    def forward(self,maximum_length,src, src_mask) :\n",
    "        src = self.pos_encoder(maximum_length,src)+src\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def generate_square_subsequent_mask(sz) :\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "\n",
    "def get_maximum_sequence_length(seq):\n",
    "    # for masking\n",
    "    length_of_seqs = []\n",
    "    for i in seq.values():\n",
    "        length_of_seqs.append(len(i))\n",
    "\n",
    "    return max(length_of_seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "from dhg.nn import HGNNConv\n",
    "from dhg import Hypergraph\n",
    "from collections import defaultdict\n",
    "\n",
    "class NeuralSeqRecommender(nn.Module):\n",
    "    def __init__(self,item_num):\n",
    "        super().__init__()\n",
    "        self.item_num = item_num\n",
    "        self.residual_gating = ResidualGAtingLayer(self.item_num,embed_size)\n",
    "        self.hconv = HGNNConv(self.item_num,embed_size,True,True,dropout_rate,True)\n",
    "        self.user_intent = ShortTermUserIntent(self.item_num,embed_size)\n",
    "        self.fusion = FusionLayer(self.item_num,embed_size)\n",
    "\n",
    "        d_hid = 200  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "        nlayers = 2  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "        nhead = 2  # number of heads in nn.MultiheadAttention\n",
    "        self.transformer = TransformerLayer(embed_size, nhead, d_hid, nlayers)\n",
    "        \n",
    "        self.all_dynamic_embeddings = []\n",
    "        self.intents = {}\n",
    "        self.interactions = defaultdict(list)\n",
    "\n",
    "    def set_btch(self,btch):        \n",
    "        self.batch_num = btch\n",
    "\n",
    "    def get_first_layer(self , static_item_embeddings , all_hyper_edges):\n",
    "        static_item_embeddings = torch.Tensor(static_item_embeddings)\n",
    "        if static_item_embeddings.shape[1] == 300:\n",
    "            static_item_embeddings = nn.Linear(300,embed_size)(static_item_embeddings)\n",
    "\n",
    "        # create dynamic embedding\n",
    "        self.all_dynamic_embeddings = []\n",
    "        dynamic_embeddings = static_item_embeddings\n",
    "        # تصحیح ایندکس ها. چون فایلها را قسمت بندی کردیم ولی ایندکس های در ادامه هم هستند . این مشکل اوت آو ایندکس میدهد\n",
    "        right_index = self.item_num*self.batch_num \n",
    "\n",
    "        for h_edge in all_hyper_edges: \n",
    "            edg = []\n",
    "            edg2 = []\n",
    "            for i in list(h_edge.values()):\n",
    "                for ii in i: \n",
    "                    edg.append(ii-right_index)\n",
    "                edg2.append(edg)\n",
    "                edg = []\n",
    "            # G = Hypergraph(self.item_num,list(h_edge.values())) #number of items(nodes) , list of edges(users)\n",
    "            G = Hypergraph(self.item_num,edg2)\n",
    "\n",
    "            dynamic_embeddings = self.hconv(dynamic_embeddings,G)\n",
    "            dynamic_embeddings = self.hconv(dynamic_embeddings,G)\n",
    "\n",
    "            self.all_dynamic_embeddings.append(dynamic_embeddings)\n",
    "\n",
    "            self.residual_gating.setter(dynamic_embeddings,static_item_embeddings)\n",
    "            dynamic_embeddings = self.residual_gating.calculate_residualGAting()\n",
    "\n",
    "        conv_dynamic_embeddings = self.all_dynamic_embeddings\n",
    "        print(\"first layer is done!\")\n",
    "        return static_item_embeddings , conv_dynamic_embeddings\n",
    "\n",
    "\n",
    "    def get_second_layer(self,all_hyper_graphs , conv_dynamic_embeddings , dataset_by_year , static_item_embeddings):\n",
    "        intents = self.create_user_intents(all_hyper_graphs , conv_dynamic_embeddings , dataset_by_year)\n",
    "        interactions = self.create_interactions(dataset_by_year , intents , conv_dynamic_embeddings , static_item_embeddings)\n",
    "        \n",
    "        print(\"second layer is done!\")\n",
    "        return interactions\n",
    "\n",
    "    def create_user_intents(self, all_hyper_graphs , conv_dynamic_embeddings , dataset_by_year):\n",
    "        for graph,embedding,aYear_data,year in zip(all_hyper_graphs,conv_dynamic_embeddings,dataset_by_year.values(),dataset_by_year.keys()):\n",
    "            userIDs = list(set(aYear_data.reviewerID))\n",
    "             \n",
    "            self.user_intent.setter(graph)\n",
    "            # intent_with_ids = defaultdict(list)\n",
    "            for u,i in zip(userIDs,self.user_intent(embedding,self.batch_num)):\n",
    "                # intent_with_ids[u].append(i)\n",
    "                self.intents[u,year]= i\n",
    "                # intent_with_ids = defaultdict(list)\n",
    "\n",
    "        return self.intents\n",
    "\n",
    "    def create_interactions(self , dataset_by_year , intents , conv_dynamic_embeddings , static_item_embeddings):\n",
    "        key_list = list(dataset_by_year.keys())\n",
    "        for year in dataset_by_year.keys():\n",
    "            df_aYear = dataset_by_year[year]\n",
    "            for interaction_row in df_aYear.iterrows():\n",
    "                right_index = self.item_num*self.batch_num \n",
    "                \n",
    "                item_id = interaction_row[1]['asin'] - right_index\n",
    "                user_id = interaction_row[1]['reviewerID']\n",
    "                y = key_list.index(year)\n",
    "                intent_tensor = intents[user_id,year]\n",
    "                self.fusion.setter(conv_dynamic_embeddings[y][item_id],static_item_embeddings[item_id],intent_tensor)\n",
    "                self.interactions[user_id,item_id,year].append(self.fusion.calculate_FusionLayer())\n",
    "\n",
    "        return self.interactions\n",
    "\n",
    "\n",
    "    def get_third_layer(self , interactions):\n",
    "        maximum_length = get_maximum_sequence_length(interactions)\n",
    "\n",
    "        src_mask = generate_square_subsequent_mask(maximum_length)\n",
    "\n",
    "        outputs = []\n",
    "        for inters in interactions.values():\n",
    "            for i in inters:\n",
    "                output = self.transformer(maximum_length,i, src_mask)\n",
    "            outputs.append(output)\n",
    "        print(\"third layer is done!\")\n",
    "        return outputs\n",
    "\n",
    "    def get_dynamic_embeddings_by_id(self,year,index):\n",
    "        return self.all_dynamic_embeddings[year][index]\n",
    "\n",
    "    def get_user_intent_by_id(self,year,index):\n",
    "        return self.intents[index,year]\n",
    "\n",
    "    def get_interactions_by_id(self,index):\n",
    "        return self.interactions[index]\n",
    "\n",
    "    def forward(self,static_item_embeddings,all_hyper_edges,all_hyper_graphs,dataset_by_year,btch):\n",
    "        self.set_btch(btch)\n",
    "        reshaped_static_embed , conv_dynamic_embeddings = self.get_first_layer(static_item_embeddings , all_hyper_edges)\n",
    "        interactions = self.get_second_layer(all_hyper_graphs , conv_dynamic_embeddings , dataset_by_year,reshaped_static_embed)\n",
    "        return reshaped_static_embed , self.get_third_layer(interactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def user_score(user_pref , dynamic_embedding , static_embedding):\n",
    "    output = dynamic_embedding + torch.Tensor(static_embedding)\n",
    "    output = torch.matmul(user_pref , output)\n",
    "    return output\n",
    "\n",
    "\n",
    "def BPR_loss_function_each_user(data,model,year,user_id,item_id,btch):\n",
    "    right_index = number_of_item*btch\n",
    "    item_id = item_id - right_index\n",
    "    all_years = list(data.get_years())\n",
    "    item_list = data.get_user_purches_item(user_id)\n",
    "    u = model.get_user_intent_by_id(year,user_id)\n",
    "    dynamic_item_embedding = model.get_dynamic_embeddings_by_id(all_years.index(year),item_id)[1]\n",
    "    static_item_embedding = reshaped_static_embed[item_id]\n",
    "    y_i = user_score(u , dynamic_item_embedding,static_item_embedding)\n",
    "\n",
    "    j_items = list(set(list(range(number_of_item))) - set(item_list))\n",
    "    j = random.choice(j_items)\n",
    "\n",
    "    dynamic_item_embedding = model.get_dynamic_embeddings_by_id(all_years.index(year),j)[1]\n",
    "    static_item_embedding = reshaped_static_embed[j]\n",
    "    y_j = user_score(u , dynamic_item_embedding,static_item_embedding)\n",
    "\n",
    "    model_parameters = torch.nn.utils.parameters_to_vector(model.parameters())\n",
    "    model_parameters_2 = torch.pow(model_parameters, 2)\n",
    "    w = torch.sum(model_parameters_2)\n",
    "\n",
    "    L2_regularization = lmbd*w\n",
    "\n",
    "    # return (y_i - y_j) - lmbd \n",
    "    return  torch.sigmoid(y_i - y_j) #- L2_regularization \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import itertools\n",
    "\n",
    "def BPR_loss_function(data,model,btch):\n",
    "    loss_list = {}\n",
    "    all_user_items = []\n",
    "    all_user_years = []\n",
    "    users = data.get_user()\n",
    "    for user in users:    \n",
    "        items = data.get_user_purches_item(user)\n",
    "        years = data.get_user_purches_year(user)\n",
    "        losses = []\n",
    "        for item , year in zip(items,years):\n",
    "            losses.append(int(BPR_loss_function_each_user(data,model,year,user,item,btch)))\n",
    "        loss_list[user]=sum(losses)\n",
    "    \n",
    "    return loss_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "import random\n",
    "\n",
    "def calculating_Ru(data , model , reshaped_static_embed , btch):\n",
    "\n",
    "    rank_of_test_data = defaultdict(list)\n",
    "    ranked_items_negative = []\n",
    "    leave_one_out_list = []\n",
    "    all_user_items = []\n",
    "    all_user_years = []\n",
    "\n",
    "    all_years = list(data.get_years())\n",
    "\n",
    "    for i in data.get_user():    \n",
    "        all_user_items.extend(data.get_user_purches_item(i))\n",
    "        all_user_years.extend(data.get_user_purches_year(i))    \n",
    "        \n",
    "        for j , k in zip(data.get_user_purches_item(i),data.get_user_purches_year(i)):\n",
    "\n",
    "            \n",
    "\n",
    "            test_index = all_user_items.index(j)\n",
    "            \n",
    "            test_item = all_user_items[test_index] \n",
    "            test_year = all_user_years[test_index]\n",
    "\n",
    "            all_user_items.remove(j)\n",
    "            all_user_years.remove(k)\n",
    "\n",
    "            #be khater namgozari behtar\n",
    "            positive_years = all_user_years\n",
    "            positive_items = all_user_items\n",
    "\n",
    "            \n",
    "            if len(positive_items) > 0 :\n",
    "                range_of_number = [item for item in range(number_of_item*btch, number_of_item*(btch+1))]\n",
    "                pItem = list(set(data.get_user_purches_item(i)))\n",
    "                for l in pItem:\n",
    "                    range_of_number.remove(l)\n",
    "                # negative_items = list(set(list(range(number_of_item))) - set(data.get_user_purches_item(i)))\n",
    "                negative_items = random.sample(range_of_number , len(positive_items))\n",
    "                negative_items.append(test_item)\n",
    "                negative_years = positive_years + [test_year]\n",
    "            else : #never happened, just for testing\n",
    "                negative_items = test_item\n",
    "                negative_years = test_year\n",
    "\n",
    "            for item,year in zip(negative_items,negative_years): \n",
    "                item = item - (number_of_item*btch)\n",
    "                u = model.get_user_intent_by_id(year,i)\n",
    "                dynamic_item_embedding = model.get_dynamic_embeddings_by_id(all_years.index(year),item)\n",
    "                static_item_embedding = reshaped_static_embed[item]\n",
    "                score = int(user_score(u , dynamic_item_embedding,static_item_embedding)) \n",
    "                item_score = (item,score)\n",
    "                ranked_items_negative.append(item_score)\n",
    "\n",
    "            ranked_items_negative = sorted(ranked_items_negative,key=itemgetter(1) , reverse=True)\n",
    "            r_u = 0\n",
    "            r_u_item = 0\n",
    "            test_item = test_item - number_of_item*btch\n",
    "            for x, y in enumerate(ranked_items_negative):\n",
    "                if y[0] == test_item :\n",
    "                    r_u = y[1] \n",
    "                    r_u_item = y[0] \n",
    "            leave_one_out_list.append(ranked_items_negative.index((r_u_item,r_u))+1)\n",
    "\n",
    "            all_user_items = []\n",
    "            all_user_years = []\n",
    "            ranked_items_negative = []\n",
    "\n",
    "            all_user_items.extend(data.get_user_purches_item(i))\n",
    "            all_user_years.extend(data.get_user_purches_year(i))\n",
    "\n",
    "        rank_of_test_data[i].append(leave_one_out_list)\n",
    "        leave_one_out_list = []\n",
    "        all_user_items = []\n",
    "        all_user_years = []\n",
    "        \n",
    "    for i,j in rank_of_test_data.items():\n",
    "        rank_of_test_data[i] = np.mean(j) \n",
    "        \n",
    "    return rank_of_test_data \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def hit_rate(r_u , k):\n",
    "    if r_u <= k:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def NDGC(r_u , k):\n",
    "    if r_u <= k:\n",
    "        return 1/math.log((r_u+1), 2)\n",
    "    else :\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_functions(rank_of_test_data):\n",
    "    hit_rate_list = defaultdict(list)\n",
    "    for i,j in rank_of_test_data.items():\n",
    "        hit_rate_list[i].append(hit_rate(j,K))\n",
    "\n",
    "    NDGC_list = defaultdict(list)\n",
    "    for i,j in rank_of_test_data.items():\n",
    "        NDGC_list[i].append(NDGC(j,K))\n",
    "\n",
    "    r_u_list = list(rank_of_test_data.values())\n",
    "    r_u_list_reverse = []\n",
    "    for i in r_u_list:\n",
    "        if i == 0:\n",
    "            r_u_list_reverse.append(0)\n",
    "        else:\n",
    "            r_u_list_reverse.append(1/i)\n",
    "    \n",
    "    return np.mean(r_u_list_reverse) , np.mean(list(NDGC_list.values())) , np.mean(list(hit_rate_list.values()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traning Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data()\n",
    "model = NeuralSeqRecommender(number_of_item)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.005,betas=(0.9,0.999),eps=1e-08,weight_decay=0,amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from collections import Counter\n",
    "for e in range(epochs):\n",
    "    print('\\n')\n",
    "    print(\"epoch {} starts :\".format(e) )\n",
    "    list_of_loss = {}\n",
    "    list_of_eval = {}\n",
    "    common_keys_loss = []\n",
    "    common_keys_eval = []\n",
    "    for i in range(0,340):\n",
    "        data.clear_every_thing()\n",
    "        data.set_data('features/feat_{}.csv'.format(i),'interactions/inter_{}.csv'.format(i))\n",
    "        data.set_static_embedding()\n",
    "        number_of_item = data.get_statistic()[0]\n",
    "        number_of_user = data.get_statistic()[1]\n",
    "        number_of_interactions = data.get_statistic()[2]\n",
    "\n",
    "        print('epoch number {} , feature file number {} : number of items: '.format(e,i),number_of_item)\n",
    "        print('epoch number {} , feature file number {}\" : number of user: '.format(e,i),number_of_user)\n",
    "        print('epoch number {} , interaction file number {}\" : number of interactions: '.format(e,i),number_of_interactions)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # model = NeuralSeqRecommender(number_of_item)\n",
    "        reshaped_static_embed , output = model(data.static_item_embedding , data.get_all_hyper_edges(),data.get_all_hyper_graphs() , data.get_dataset_by_year(),i)\n",
    "\n",
    "        #loss\n",
    "        if i==0 :\n",
    "            list_of_loss= BPR_loss_function(data,model,i)\n",
    "        else:\n",
    "            tempListLoss = BPR_loss_function(data,model,i)\n",
    "            all_loss = dict(Counter(list_of_loss) + Counter(tempListLoss))\n",
    "            common_keys_loss.append(set(all_loss).intersection(tempListLoss))\n",
    "            \n",
    "            list_of_loss = {}\n",
    "            list_of_loss = all_loss.copy()\n",
    "            all_loss = {} \n",
    "        \n",
    "            \n",
    "        \n",
    "\n",
    "        #evaluations\n",
    "        if i==0 :\n",
    "            list_of_eval= calculating_Ru(data , model , reshaped_static_embed,i)\n",
    "        else:\n",
    "            tempList = calculating_Ru(data , model , reshaped_static_embed,i)\n",
    "            all_evaluation = dict(Counter(list_of_eval) + Counter(tempList))\n",
    "            common_keys_eval.append(set(all_evaluation).intersection(tempList))\n",
    "            \n",
    "            list_of_eval = {}\n",
    "            list_of_eval = all_evaluation.copy()\n",
    "            all_evaluation = {}\n",
    "\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "   # loss\n",
    "    lcommon_keys = []\n",
    "    for i in common_keys_loss:\n",
    "        for j in i:\n",
    "            lcommon_keys.append(j)\n",
    "    lcommon_keys = list(set(lcommon_keys))\n",
    "    for k in lcommon_keys:\n",
    "        list_of_loss[k]=list_of_loss[k]/5\n",
    "\n",
    "    u = 0\n",
    "    for ii in list_of_loss.values():\n",
    "        u +=ii\n",
    "    u = u/len(list_of_loss)\n",
    "    print('epoch {} : loss = '.format(e)  , u)\n",
    "\n",
    "    #evaluation\n",
    "    ecommon_keys = []\n",
    "    for i in common_keys_eval:\n",
    "        for j in i:\n",
    "            ecommon_keys.append(j)\n",
    "    ecommon_keys = list(set(ecommon_keys))\n",
    "    for k in ecommon_keys:\n",
    "        list_of_eval[k]=list_of_eval[k]/5\n",
    "    evaluation_result = eval_functions(list_of_eval)\n",
    "\n",
    "\n",
    "    print('epoch {} : MRR'.format(e) ,evaluation_result[0])\n",
    "    print('epoch {} : NDGC@3'.format(e) ,evaluation_result[1])\n",
    "    print('epoch {} : hit_rate@3'.format(e) ,evaluation_result[2])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "939480ed579cbcc9bd95c0bb2f0a271d068ec362d36f1415ed941c7dadb52340"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
